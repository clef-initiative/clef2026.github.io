---
title: "LongEval"
draft: false
summary: "Longitudinal evaluation of model performance"
menu:
  main:
    identifier: "lab-longeval"
    parent: "labs"
    weight: 110
---

## Overview

Most Information Retrieval (IR) benchmarks evaluate systems at a single point in time, despite data and user behaviors changing over time. Research shows that IR and text classification systems lose effectiveness as data patterns evolve, especially when test data is temporally distant from training data. This lab encourages developing models that maintain performance over time by providing training and testing data from different periods. We propose the fourth LongEval Lab to further focus on evaluating IR systems' ability to generalize across time, using datasets split by various temporal distances to assess how well systems handle evolving documents and queries. For 2026 we plan a total of 4 tasks, widening the scope of long-term IR to new dynamics beyond documents, topics and qrels, closer to evolving user behavior with user simulation tasks.

## Organizers

- Matteo Cancellieri
- Alaa El-Ebshihy
- Maik Fröbe
- Petra Galuščáková
- Gabriela González Sáez
- Lorraine Goeuriot
- Gabriel Iturra-Bocaz
- Jüri Keller
- Petr Knoth
- Philippe Mulhem
- Florina Piroi
- David Pride
- Philipp Schaer
- Didier Schwab
